{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675291e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70998a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fd071",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "raw_data_path = \"/content/drive/MyDrive/Colab Notebooks/audio_triplet_dataset/spectogrammes\"\n",
    "actors = sorted(os.listdir(raw_data_path))\n",
    "\n",
    "# --- Split actors en Train / Test ---\n",
    "train_actors, test_actors = train_test_split(\n",
    "    actors, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train actors:\", len(train_actors))\n",
    "print(\"Test actors :\", len(test_actors))\n",
    "\n",
    "def load_images(actor_list):\n",
    "    images_by_actor = {}\n",
    "    for actor in actor_list:\n",
    "        actor_dir = os.path.join(raw_data_path, actor)\n",
    "        images = [\n",
    "            os.path.join(actor_dir, f) \n",
    "            for f in os.listdir(actor_dir) \n",
    "            if f.endswith(\".png\")\n",
    "        ]\n",
    "        if len(images) > 1:\n",
    "            images_by_actor[actor] = images\n",
    "    return images_by_actor\n",
    "\n",
    "\n",
    "train_images_by_actor = load_images(train_actors)\n",
    "test_images_by_actor = load_images(test_actors)\n",
    "\n",
    "\n",
    "def generate_triplets(images_by_actor, all_actors):\n",
    "    triplets = []\n",
    "    for actor, images in images_by_actor.items():\n",
    "        other_actors = [a for a in all_actors if a != actor]\n",
    "        max_pairs = min(75, len(images) * len(images))\n",
    "        for _ in range(max_pairs):\n",
    "            anchor, positive = np.random.choice(images, 2, replace=False)\n",
    "            negative_actor = np.random.choice(other_actors)\n",
    "            negative = np.random.choice(images_by_actor[negative_actor])\n",
    "            triplets.append((anchor, positive, negative))\n",
    "    \n",
    "    return triplets\n",
    "\n",
    "\n",
    "print(\"\\nGenerating TRAIN triplets...\")\n",
    "train_triplets = generate_triplets(train_images_by_actor, train_actors)\n",
    "\n",
    "print(\"Generating TEST triplets...\")\n",
    "test_triplets = generate_triplets(test_images_by_actor, test_actors)\n",
    "\n",
    "df_train = pd.DataFrame(train_triplets, columns=[\"anchor\", \"positive\", \"negative\"])\n",
    "df_train.to_csv(\"/content/drive/MyDrive/Colab Notebooks/audio_triplet_dataset/train_triplet/train_triplets.csv\", index=False)\n",
    "\n",
    "df_test = pd.DataFrame(test_triplets, columns=[\"anchor\", \"positive\", \"negative\"])\n",
    "df_test.to_csv(\"/content/drive/MyDrive/Colab Notebooks/audio_triplet_dataset/test_triplet/test_triplets.csv\", index=False)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nTime taken: {end - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc24a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d( in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "class SpeakerEmbeddingCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        # --- Feature extractor (4 blocks CNN)\n",
    "        self.features = nn.Sequential(\n",
    "            ConvBlock(3, 32),   \n",
    "            ConvBlock(32, 64),\n",
    "            ConvBlock(64, 128), \n",
    "            ConvBlock(128, 256)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Output: 256 × 1 × 1\n",
    "        self.embedding_fc = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Fully connected embedding\n",
    "        x = self.embedding_fc(x)\n",
    "        # 4) Normalize embeddings for triplet loss\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# training dataset preparation\n",
    "transform = T.Compose([T.Resize((224, 224)), T.ToTensor(), T.Normalize(mean=[0.5], std=[0.5])])\n",
    "train_dataset = TripletDataset(csv_file=\"/content/drive/MyDrive/Colab Notebooks/audio_triplet_dataset/train_triplet/train_triplets.csv\",transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# model preparation\n",
    "model = SpeakerEmbeddingCNN(embedding_dim=128).to(device)\n",
    "criterion = nn.TripletMarginLoss(margin= 0.2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "start = time.time()\n",
    "for epoch in range(10):\n",
    "    start_epoch = time.time()  \n",
    "    print(\"start training...\")\n",
    "    for anchor, positive, negative in train_loader:\n",
    "        print(\"batch...\")\n",
    "        start_batch = time.time()\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "\n",
    "        anchor_emb = model(anchor)\n",
    "        positive_emb = model(positive)\n",
    "        negative_emb = model(negative)\n",
    "\n",
    "        loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "\n",
    "        # 3) Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_batch = time.time()\n",
    "        print(f\"Batch time: {end_batch - start_batch:.2f} seconds\")\n",
    "    print(f\"Epoch {epoch} — Loss={loss.item():.4f}\")\n",
    "    end_epoch = time.time()\n",
    "    print(f\"Epoch time: {end_epoch - start_epoch:.2f} seconds\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nTraining time: {end - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18465f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/content/saved_models/speaker_model.pth\")\n",
    "model_path = \"/content/saved_models/speaker_model.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SpeakerEmbeddingCNN(embedding_dim=64).to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TripletDataset(csv_file=\"/content/drive/MyDrive/Colab Notebooks/audio_triplet_dataset/test_triplet/test_triplets.csv\",transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "all_distances = []\n",
    "all_labels = []  # 1 si même speaker, 0 si différent\n",
    "with torch.no_grad():\n",
    "    print(\"start testing...\")\n",
    "    for anchor, positive, negative in test_loader:\n",
    "        print(\"test batch...\")\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "\n",
    "        anchor_emb = model(anchor)\n",
    "        positive_emb = model(positive)\n",
    "        negative_emb = model(negative)\n",
    "        # Cosine distance\n",
    "        pos_dist = F.cosine_similarity(anchor_emb, positive_emb).cpu().item()\n",
    "        neg_dist = F.cosine_similarity(anchor_emb, negative_emb).cpu().item()\n",
    "\n",
    "        all_distances.append((pos_dist, neg_dist))\n",
    "        all_labels.append((1, 0))  # 1 pour positive, 0 pour negative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
